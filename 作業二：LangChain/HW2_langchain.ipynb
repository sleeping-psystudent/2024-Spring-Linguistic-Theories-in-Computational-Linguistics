{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAGLpn96Qxyf"
      },
      "source": [
        "# 作業二：LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5ggnHOphR97"
      },
      "source": [
        "## 請先安裝以下 (可能用得到的) packages\n",
        "\n",
        "<mark>Please  install more packages if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jow5c8X5hU-Y"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q python-dotenv\n",
        "!pip install -U -q openai\n",
        "!pip install -U -q langchain\n",
        "!pip install -U -q langchain_openai\n",
        "!pip install -U -q langchain_community\n",
        "!pip install -U -q langchain_experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wgq36Je08Qte"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q langchain-text-splitters\n",
        "!pip install -U -q chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPwftnogE9gP",
        "outputId": "8ebec91b-2241-4358-cc1d-50af1a6b287f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6V8q4h2x0oS5"
      },
      "outputs": [],
      "source": [
        "# 將當前目錄設定成 .env 存放的那個資料夾\n",
        "import os\n",
        "os.chdir('/content/gdrive/MyDrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9815081",
        "outputId": "837dacb2-770a-4ad7-ab8c-b197d74db6d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "有讀到 HUGGINGFACEHUB_API key\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv('HUGGINGFACEHUB_API_TOKEN', '沒讀到 HUGGINGFACEHUB API key')\n",
        "\n",
        "# 失敗的話會列印出：'沒讀到 HUGGINGFACEHUB_API key'\n",
        "api_token = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
        "\n",
        "# 不希望API key值被印出所以多加一道檢查\n",
        "if api_token != '沒讀到 HUGGINGFACEHUB API key':\n",
        "    print(\"有讀到 HUGGINGFACEHUB_API key\")\n",
        "else:\n",
        "    print(api_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk22LEuba98I"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Part 1: Building a Simple QA App [25分]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruBlVuI4bAj1"
      },
      "source": [
        "🔺請依照以下指示，創建 1 個 LLMchain，並用其回答指定問題：\n",
        "\n",
        "1. **Install and import necessary libraries**\n",
        "2. **Define an LLM** <5分>\n",
        "  - 定義一個你想使用的模型\n",
        "  - Either an LLM model or a Chat model\n",
        "3. **Define a Prompt** <5分>\n",
        "  - 使用 `PromptTemplate` or `ChatPromptTemplate` 創建一個 prompt\n",
        "  - 這個 prompt 必須包含變數 `question`\n",
        "4. **Define a Chain** <5分>\n",
        "  - 使用剛剛定義的 LLM & prompt 創建一個 chain\n",
        "5. **Invoke the Chain & Print the Answer** <10分>\n",
        "  - <mark>請用這個問題：`\"Are there aliens on the Moon?\"`\n",
        "  - 需用到 `.invoke()`\n",
        "  - 修改問號處並使用以下程式碼印出結果：\n",
        "\n",
        "    ```\n",
        "    answer = ???????\n",
        "    print(answer.content)\n",
        "    ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LN3qNQ2GJGOO"
      },
      "outputs": [],
      "source": [
        "# 1. Install and import necessary libraries\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIAw0uCcJL4e",
        "outputId": "2088b427-c489-412e-c822-443ba9ec35d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "# 2. 定義模型\n",
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "llm_try = HuggingFaceEndpoint(repo_id=repo_id, temperature = 0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vvhDEufEx5F2"
      },
      "outputs": [],
      "source": [
        "# 3. 定義 prompt\n",
        "template = \"\"\"\n",
        "You are now an extraterrestrial enthusiast, firmly believing in the existence of aliens on the moon. Please reply to \"{question}\"\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QYxKUNU-8bNr"
      },
      "outputs": [],
      "source": [
        "# 4. 定義 chain\n",
        "chain = prompt | llm_try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBvp3qYG84hc",
        "outputId": "ba580b04-53b8-4b1b-f69c-a359705d64b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Subject: 🌕 Extraterrestrial Moon Discoveries 👽\n",
            "\n",
            "Hi there! I'm thrilled that you've asked about the possibility of aliens on the moon. Given my newfound interest in all things extraterrestrial, I've been keeping a close eye on the latest discoveries and theories.\n",
            "\n",
            "There have indeed been several intriguing findings over the years that have fueled speculation about lunar life. Here are a few:\n",
            "\n",
            "1. Anomalous Lunar Transient Phenomena (ALTP): These are unexplained bright spots or flashes observed on the lunar surface by various observatories. Some researchers believe they could be signs of subsurface water or even artificial structures.\n",
            "2. The Far Side Moon Mystery: The far side of the moon, which is always facing away from Earth, has no permanent human presence. However, there have been reports of strange structures and formations visible only from this side. Some theories suggest these could be remnants of an ancient lunar civilization.\n",
            "3. The Cydonia Region: This area on the moon is famous for its \"Face on Mars,\" but it also contains several other intriguing formations, such as the \"Moon Pyramids\" and the \"Moon Dome.\" Some believe these structures could be evidence of past or present lunar life.\n",
            "4. Radio Signals: In the late 1960s and early 1970s, several radio telescopes picked up strange signals from space that seemed to originate near the moon. These signals have since been explained as terrestrial interference or natural phenomena, but some researchers still believe they could have been of extraterrestrial origin.\n",
            "5. Worm Holes and Portals: While purely speculative, some theories suggest that aliens could be using worm holes or portals to travel to and from the moon. There have been numerous reports of strange phenomena in the area around the moon, including unidentified flying objects and unexplained lights.\n",
            "\n",
            "Of course, none of these findings have been definitively proven to be of extraterrestrial origin, and many scientists remain skeptical. But for those of us who believe, the mystery of the moon only deepens with each new discovery.\n",
            "\n",
            "So, to answer your question, are there aliens on the moon? I believe\n"
          ]
        }
      ],
      "source": [
        "# 5.6 Invoke the Chain & 列印結果\n",
        "answer = chain.invoke({\"question\": \"Are there aliens on the Moon?\"})\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijUhMdJx00tM"
      },
      "source": [
        "### 參考資料\n",
        "- [[D15] LangChain 專案實做 - Hello LangChain](https://ithelp.ithome.com.tw/articles/10322165?fbclid=IwZXh0bgNhZW0CMTAAAR3uGLgZxX6ZUKXFzGHlCflUcIXDk3AWqjNLtICRzgqkGEmisQd4I76ar9k_aem_AYLjV9jct7fZVFaRjypjts1SfF1rnX5W4sd_cjLeAktZeNtxyxGfUv1mksXGgLXcvN0C29sJuAkXiIIH13RXXULE)\n",
        "- [【初學筆記】取得Hugging Face API Tokens 到執行簡單LLM問答內容生成(Inference)](https://medium.com/@Ting_YTK/%E5%88%9D%E5%AD%B8%E7%AD%86%E8%A8%98-%E5%8F%96%E5%BE%97hugging-face-api-tokens-%E5%88%B0%E5%9F%B7%E8%A1%8C%E7%B0%A1%E5%96%AEllm%E5%95%8F%E7%AD%94%E5%85%A7%E5%AE%B9%E7%94%9F%E6%88%90-inference-3473baa347f6)\n",
        "- [LLM Note Day 23 - LangChain 中二技能翻譯](https://ithelp.ithome.com.tw/articles/10335721)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5x_sO1-eV2l"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Part 2: Customize 3 Math Tools [30分]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtjruBg_iauZ"
      },
      "source": [
        "🔺請依照以下指示，創建 3 個 LangChain 工具 for 基本數學運算：\n",
        "\n",
        "1. **Install and import necessary libraries**\n",
        "2. **Define Tools**\n",
        "\n",
        "    - 請將三個工具分別命名成： `divide`, `minus`, `exponentiate`\n",
        "    - 使用 `langchain_core.tools` 模組中的 @tool decorator 來定義每個工具，需定義每個工具的參數和回傳類型 <18分>\n",
        "    - 在工具的函數中編寫程式碼，以執行特定的數學運算 <6分>\n",
        "\n",
        "        🌟HINTS🌟\n",
        "      - divide: 第1個整數 除以 第2個整數\n",
        "      - minus: 第1個整數 減 第2個整數\n",
        "      - exponentiate: 第1個整數的(第2個整數)次方\n",
        "\n",
        "3. **Store the Tools** <6分>\n",
        "  - 請將三個工具以 list 方式，儲存在 `tools` 這個變數"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1kGh5ar0gIxN"
      },
      "outputs": [],
      "source": [
        "## 1. Install and import necessary libraries\n",
        "from langchain_core.tools import tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dqd5_lFfgZT9"
      },
      "outputs": [],
      "source": [
        "## 2. Define Tools\n",
        "@tool\n",
        "def divide(a: int, b:int) -> float:\n",
        "  \"\"\"Divide the first integer by the second integer.\"\"\"\n",
        "  return a/b\n",
        "\n",
        "@tool\n",
        "def minus(a: int, b:int) -> int:\n",
        "  \"\"\"Subtract the second integer from the first integer.\"\"\"\n",
        "  return a-b\n",
        "\n",
        "@tool\n",
        "def exponentiate(a: int, b:int) -> float:\n",
        "  \"\"\"The first integer raised to the power of the second integer.\"\"\"\n",
        "  return a**b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "t0a-VTFoJapR"
      },
      "outputs": [],
      "source": [
        "## 3. Store the Tools\n",
        "tools = [divide, minus, exponentiate]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFLhceWRVAoE"
      },
      "source": [
        "### 參考資料\n",
        "\n",
        "- [最全面又最浅显易懂的Langchain快速上手教程（下）](https://juejin.cn/post/7345869813329936399)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzyF5dqzeYeV"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Part 3: Combine Tools with Agent [45分]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmOrxYjSmTx4"
      },
      "source": [
        "🔺請依照以下指示，結合 Part 2 的 `tools` 創建 1 個 agent\n",
        "0. **請保留 Part 2 儲存的 `tools`**\n",
        "  - <mark>如果 part 2 part 3 分成兩次做，請先跑一次 part 2 的 code，再繼續執行以下操作\n",
        "1. **Install and import necessary libraries**\n",
        "2. **Define an LLM** <5分>\n",
        "  - 選用一個 HuggingFace 模型\n",
        "\n",
        "\n",
        "3. **Define Prompt** <20分>\n",
        "  - 使用 `ChatPromptTemplate.from_messages()` 創造一個 prompt <\n",
        "  - 這格程式碼會長得像：\n",
        "    ```python\n",
        "      prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "          (\"¿¿\", \"❓❓❓\",),\n",
        "          (\"placeholder\", \"❓❓❓\"),\n",
        "          (\"¿¿\", \"❓❓❓\"),\n",
        "          (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "        ]\n",
        "      )\n",
        "    ```\n",
        "  - 當然你也可以自由發揮🐳\n",
        "  \n",
        "4. **Define Agent Chain** <15分>\n",
        "  - 使用 `initialize_agent()` 定義一個 agent chain\n",
        "  - <mark>可能需要的模組和函數\n",
        "    ```python\n",
        "      from langchain.agents import AgentType\n",
        "      from langchain.agents import initialize_agent\n",
        "    ```\n",
        "5. **Invoke Agent Executor** <5分>\n",
        "  - 使用以下程式碼印出模型的回覆結果\n",
        "    ```python\n",
        "      ## invoke agent_executor 列印模型的回覆結果\n",
        "      result = agent_executor.invoke({\n",
        "      \"input\": \"Take 2 to the power of 6, then divide this result obtained by the result of 24 minus 8, then square the whole result\"\n",
        "      })\n",
        "\n",
        "      print(result)\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "neyNksnvI7vg"
      },
      "outputs": [],
      "source": [
        "# 1. Install and import necessary libraries\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain.agents import AgentType\n",
        "from langchain.agents import initialize_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKiZ1xioPs6V",
        "outputId": "851ee534-48fd-441c-ade1-0b60595f304b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "# 2. 定義 LLM\n",
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "llm = HuggingFaceEndpoint(repo_id=repo_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5j1_KEEKgg05"
      },
      "outputs": [],
      "source": [
        "# 3. 定義 prompt\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "   [\n",
        "     (\"user\", \"I have a question. {input}. Please carefully find the correct algorithm.\"),\n",
        "     (\"assistant\", \"Okay, let me tell you the answer.\")\n",
        "   ]\n",
        " )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhiEgFg0962b",
        "outputId": "46bd653f-1315-4102-c5f9-a3ce4e7b7771"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "# 4. 定義 agent_chain\n",
        "agent_chain = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    handle_parsing_errors=True,\n",
        "    verbose=True\n",
        ")\n",
        "agent_executor = prompt | agent_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiyUOSHxGFLc",
        "outputId": "30400609-04a9-450d-f149-6906ceb2ff5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction:\n",
            "```json\n",
            "{\n",
            "  \"action\": \"divide\",\n",
            "  \"action_input\": {\n",
            "    \"a\": 64,\n",
            "    \"b\": 16\n",
            "  }\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m4.0\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to subtract 8 from 24 first.\n",
            "Action:\n",
            "```json\n",
            "{\n",
            "  \"action\": \"minus\",\n",
            "  \"action_input\": {\n",
            "    \"a\": 24,\n",
            "    \"b\": 8\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m16\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m Now I can find 2 raised to the power of 6 and divide it by 16.\n",
            "Action:\n",
            "```json\n",
            "{\n",
            "  \"action\": \"exponentiate\",\n",
            "  \"action_input\": {\n",
            "    \"a\": 2,\n",
            "    \"b\": 6\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m64\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m Now I can divide 64 by 16.\n",
            "Action:\n",
            "```json\n",
            "{\n",
            "  \"action\": \"divide\",\n",
            "  \"action_input\": {\n",
            "    \"a\": 64,\n",
            "    \"b\": 16\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m4.0\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m Now I need to square 4.\n",
            "Action:\n",
            "```json\n",
            "{\n",
            "  \"action\": \"exponentiate\",\n",
            "  \"action_input\": {\n",
            "    \"a\": 4,\n",
            "    \"b\": 2\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m16\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I know what to respond.\n",
            "Action:\n",
            "```json\n",
            "{\n",
            "  \"action\": \"Final Answer\",\n",
            "  \"action_input\": \"The final answer to your question is 16.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': ChatPromptValue(messages=[HumanMessage(content='I have a question. Take 2 to the power of 6, then divide this result obtained by the result of 24 minus 8, then square the whole result. Please carefully find the correct algorithm.'), AIMessage(content='Okay, let me tell you the answer.')]), 'output': 'The final answer to your question is 16.'}\n"
          ]
        }
      ],
      "source": [
        "# 5. 列印模型的回覆結果\n",
        "result = agent_executor.invoke({\n",
        "    \"input\": \"Take 2 to the power of 6, then divide this result obtained by the result of 24 minus 8, then square the whole result\"\n",
        " })\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFbXR8C3auc9"
      },
      "source": [
        "### 參考資料\n",
        "\n",
        "- [[LangChain for LLM Application Development] 課程筆記- Agents](https://hackmd.io/@YungHuiHsu/rkBMDgRM6?utm_source=preview-mode&utm_medium=rec)\n",
        "- [LangChain 9 模型Model I/O 聊天提示词ChatPromptTemplate, 少量样本提示词FewShotPrompt](https://blog.csdn.net/zgpeace/article/details/134608897)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbls4nZXMCpM"
      },
      "source": [
        "## 🌟BONUS: 針對 YT 影片問問題 [20分]🌟"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0S8S35DMFCE"
      },
      "source": [
        "1. 使用 [YouTube transcripts Loaders](https://python.langchain.com/v0.1/docs/integrations/document_loaders/youtube_transcript/) 抓取任何「長度大於10分鐘」的 YouTube影片的cc字幕，會讀成一個長文本\n",
        "2. 將此文本做成向量，存到一個 VectorStore DB （建議使用 [Chroma](https://python.langchain.com/v0.1/docs/integrations/vectorstores/chroma/)）\n",
        "3. 建立一個 retriever 以及基於此 retriever 的 LLM chain（[助教課的Colab](https://colab.research.google.com/drive/1T7Q4X9qeppvMuYI3pJLN4OfD9L_F6YMx?usp=sharing)有教ㄛ🐳）\n",
        "4. 設計一個和此影片有關的問題\n",
        "5. 最後請把  **`chain.invoke('貼上你自己設計的問題')`  的結果列印出來**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Db2vwsb38TcT"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet  youtube-transcript-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DuEhEXP3S9sn"
      },
      "outputs": [],
      "source": [
        "# Install and import necessary libraries\n",
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnjuDTBa71Vn",
        "outputId": "5ce94933-ecd0-41d1-9321-f32cc0e92535"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "我們有 4 splitted documents\n",
            "平均每個 splitted document 有 2,586 characters\n"
          ]
        }
      ],
      "source": [
        "# YouTube transcripts Loaders\n",
        "loader = YoutubeLoader.from_youtube_url(\n",
        "    \"https://www.youtube.com/watch?v=lYtdZo0aW50\", add_video_info=False\n",
        ")\n",
        "film = loader.load()\n",
        "\n",
        "# Split the text\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=100)\n",
        "sp_film = text_splitter.split_documents(film)\n",
        "\n",
        "num_total_characters = sum([len(x.page_content) for x in sp_film])\n",
        "\n",
        "print(f\"我們有 {len(sp_film)} splitted documents\")\n",
        "print(f\"平均每個 splitted document 有 {num_total_characters / len(sp_film):,.0f} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFc8F6DJ9UI1",
        "outputId": "07918896-618a-4245-a9d3-656b6350ddc0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['459a7d18-5de8-417b-804d-786cba33fcc0',\n",
              " 'b8763bde-3966-46d4-b0f4-40614cbaf8de',\n",
              " '70188cc7-b140-4e18-b7c6-fad7e3075d54',\n",
              " 'a171c430-cc42-425a-a37d-4e7e5a1cc8f3']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the embeddings engine ready\n",
        "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-l6-v2\",\n",
        "    api_key=api_token,\n",
        ")\n",
        "\n",
        "# Set the vectorstore\n",
        "vectorstoreDB = Chroma(embedding_function = embeddings,\n",
        "                        persist_directory = './Chroma_DATABASE')\n",
        "vectorstoreDB.add_documents(sp_film)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "S3ZVy0xT98Es"
      },
      "outputs": [],
      "source": [
        "# Define retriever\n",
        "retriever = vectorstoreDB.as_retriever(search_kwargs={\"k\": 1})\n",
        "\n",
        "## Define the prompt\n",
        "template = \"\"\"Please answer the following question based on the context provided:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Set the LLM chain\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WVGiuWNl_G8Z"
      },
      "outputs": [],
      "source": [
        "# Question\n",
        "user_query = \"How did the speaker rebel when he was 14?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "fia2fOiy_LkR",
        "outputId": "3d294cd8-e81b-4bb4-ef7f-d9ce4b9daaab"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nAnswer: The speaker rebelled by doing his math homework in secret, locking himself in a room, and hiding it from his father. When his father discovered him, the speaker lied and claimed he was masturbating instead, hoping his father would not be disappointed in him for doing homework. The speaker also mentioned that his parents are very traditional and hard to please, making it difficult for him to bond with them or enjoy simple pleasures like watching TV.'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Invoke the chain\n",
        "chain.invoke(user_query)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
